{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "b10fc941-828d-4d27-9534-c1fc2689edbe",
      "metadata": {},
      "outputs": [],
      "source": ["%load_ext lab_black"]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "4ed301a0-6865-4d36-99d6-eb4e858fac3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"Lidl\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "e5694a68-7c62-440f-a4fa-e69200e6bd2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "path = \"./events.csv\"\n",
        "schema = StructType(\n",
        "    [\n",
        "        StructField(\"time\", TimestampType()),\n",
        "        StructField(\"action\", StringType()),\n",
        "    ]\n",
        ")\n",
        "df = spark.read.schema(schema).csv(path, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "112348ec-12f4-4ce3-954c-2afa226cd6a0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----+----+\n",
            "|              window|Close|Open|\n",
            "+--------------------+-----+----+\n",
            "|{2016-07-26 02:40...| null|  32|\n",
            "|{2016-07-26 02:50...|   11| 147|\n",
            "|{2016-07-26 03:00...|   19| 162|\n",
            "|{2016-07-26 03:10...|   42| 169|\n",
            "|{2016-07-26 03:20...|   44| 170|\n",
            "|{2016-07-26 03:30...|   58| 157|\n",
            "|{2016-07-26 03:40...|   84| 173|\n",
            "|{2016-07-26 03:50...|   97| 170|\n",
            "|{2016-07-26 04:00...|   96| 165|\n",
            "|{2016-07-26 04:10...|  109| 164|\n",
            "|{2016-07-26 04:20...|  137| 164|\n",
            "|{2016-07-26 04:30...|  172| 167|\n",
            "|{2016-07-26 04:40...|  142| 170|\n",
            "|{2016-07-26 04:50...|  159| 169|\n",
            "|{2016-07-26 05:00...|  170| 166|\n",
            "|{2016-07-26 05:10...|  165| 169|\n",
            "|{2016-07-26 05:20...|  179| 167|\n",
            "|{2016-07-26 05:30...|  158| 153|\n",
            "|{2016-07-26 05:40...|  181| 176|\n",
            "|{2016-07-26 05:50...|  150| 169|\n",
            "+--------------------+-----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "WINDOW_SIZE = 10\n",
        "# 10 minute aggregation\n",
        "ten_minutes = (\n",
        "    df.groupBy(window(\"time\", f\"{WINDOW_SIZE} minute\")).pivot(\"action\").count()\n",
        ")\n",
        "ten_minutes.orderBy(asc(\"window\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "12fdaa26-d0e7-4942-ac09-e10d9c3bc397",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------------+-----------+\n",
            "|              window|Close/Minute|Open/Minute|\n",
            "+--------------------+------------+-----------+\n",
            "|{2016-07-27 06:00...|        18.4|       17.9|\n",
            "|{2016-07-26 05:40...|        18.1|       17.6|\n",
            "|{2016-07-27 00:00...|        17.0|       18.4|\n",
            "|{2016-07-27 10:30...|        15.6|       16.5|\n",
            "|{2016-07-26 19:10...|        19.3|       16.8|\n",
            "|{2016-07-28 04:30...|        14.9|       17.2|\n",
            "|{2016-07-27 05:10...|        15.3|       16.9|\n",
            "|{2016-07-27 01:10...|        16.8|       17.4|\n",
            "|{2016-07-27 18:20...|        19.3|       16.9|\n",
            "|{2016-07-26 23:30...|        13.8|       17.1|\n",
            "|{2016-07-27 10:50...|        17.5|       16.7|\n",
            "|{2016-07-26 08:10...|        14.4|       15.5|\n",
            "|{2016-07-26 23:10...|        16.3|       17.1|\n",
            "|{2016-07-27 12:30...|        17.9|       17.4|\n",
            "|{2016-07-27 23:10...|        16.9|       16.6|\n",
            "|{2016-07-27 16:20...|        15.3|       16.3|\n",
            "|{2016-07-27 01:40...|        17.9|       15.8|\n",
            "|{2016-07-27 08:30...|        18.2|       15.5|\n",
            "|{2016-07-27 00:50...|        16.6|       16.6|\n",
            "|{2016-07-28 03:20...|        16.2|       17.1|\n",
            "+--------------------+------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 10 minute aggregation per minute\n",
        "per_minute_aggregation = (\n",
        "    ten_minutes.withColumn(\"Close/Minute\", col(\"Close\") / WINDOW_SIZE)\n",
        "    .withColumn(\"Open/Minute\", col(\"Open\") / WINDOW_SIZE)\n",
        "    .drop(\"Open\", \"Close\")\n",
        ")\n",
        "per_minute_aggregation.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "b403af55-f73c-4ea9-8483-8777af16761f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+------------------+\n",
            "|         avg(Open)|        avg(Close)|\n",
            "+------------------+------------------+\n",
            "|165.56291390728478|160.25641025641025|\n",
            "+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# compute the average number of actions each 10 minutes.\n",
        "close_average = ten_minutes.select(\n",
        "    mean(\"Open\"),\n",
        "    mean(\"Close\"),\n",
        ").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "64284c73-454c-4386-a6ec-b6cf5519a33a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----+----+----+\n",
            "|              window|Close|Open|diff|\n",
            "+--------------------+-----+----+----+\n",
            "|{2016-07-26 03:00...|   19| 162| 143|\n",
            "|{2016-07-26 02:50...|   11| 147| 136|\n",
            "|{2016-07-26 03:10...|   42| 169| 127|\n",
            "|{2016-07-26 03:20...|   44| 170| 126|\n",
            "|{2016-07-26 03:30...|   58| 157|  99|\n",
            "|{2016-07-26 03:40...|   84| 173|  89|\n",
            "|{2016-07-26 03:50...|   97| 170|  73|\n",
            "|{2016-07-26 04:00...|   96| 165|  69|\n",
            "|{2016-07-26 04:10...|  109| 164|  55|\n",
            "|{2016-07-27 21:20...|  127| 176|  49|\n",
            "+--------------------+-----+----+----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# compute the top 10 minutes with a bigger amount of \"open\" action.\n",
        "ten_minutes.withColumn(\"diff\", col(\"Open\") - col(\"Close\")).orderBy(desc(\"diff\")).show(\n",
        "    10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6189f652-b79d-4d00-83b7-afaa2d750607",
      "metadata": {
        "tags": []
      },
      "source": [
        "### Unit Test Proposal\n",
        "\n",
        "Using pytest/unitest packages it is trivial to generate unit tests for spark dataframes. \n",
        "Simple define some assertions about your data,for example, that action type is always a value in set(\"Open\", \"Close\") \n",
        "and run an assert statement such as:\n",
        "\n",
        "```python\n",
        "assert set([\"Open\", \"Close\"]) == set(\n",
        "    [x.action for x in df.select(\"action\").distinct().collect()]\n",
        ")\n",
        "\n",
        "```\n",
        "\n",
        "Another assertion could be related to the time values ensuring that timestamps are within an expected date-range.\n",
        "\n",
        "### Integration Testing\n",
        "\n",
        "Typically every transformation you apply to the spark df should have a unit test for the logic. But every ETL pipeline also includes\n",
        "reading data from a datasource and writing data to a database. It is these \"integration\" points that need to be tested in \n",
        "integration tests. For example, what happens if the data is malformed in the source csv, how does your code handle these errors.\n",
        "\n",
        "Step one is define a set of immuatable test data with known features and then simpley run the transformations against the known data against the expected outcome. For example, provide a the `spark.read.csv` method call a path to a csv file that has an invalid schema and expect to receive an error.\n",
        "\n",
        "### Test Suite Implemntation\n",
        "\n",
        "Generally speaking tests should be ran at the point of raising a PR. Nothing should merged into the main/master branch without first passing the test suite. Most modern git platforms, github/gitlab etc have CI/CD actions built into the platform. Dockerizing the spark application and running the tests as a pre-requisite to merge acceptance is the generally accepted method of running tests.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
